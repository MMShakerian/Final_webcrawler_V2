# خزنده وب با Scrapy

این پروژه یک خزنده وب مبتنی بر Scrapy است که وب‌سایت‌های مشخص را خزش می‌کند و ساختار لینک‌های داخلی آن را استخراج و به صورت درختی نمایش می‌دهد.

## ویژگی‌ها

- استخراج لینک‌های داخلی از یک وب‌سایت داده شده
- پیگیری لینک‌ها تا عمق مشخص (به طور پیش‌فرض 3 سطح)
- شناسایی لینک‌های خارجی (بدون پیگیری آن‌ها)
- ایجاد گزارش به صورت ساختار درختی
- ذخیره نتایج در فرمت‌های متنی (report.txt) و JSON (tree.json)

## نیازمندی‌ها

- Python 3.7+
- Scrapy

## نصب

1. کلون کردن مخزن:
```
git clone https://github.com/yourusername/mycrawler.git
cd mycrawler
```

2. نصب وابستگی‌ها:
```
pip install scrapy
```

## استفاده

برای شروع خزش یک وب‌سایت، دستور زیر را اجرا کنید:

```
scrapy crawl link_spider -a start_url=https://example.com
```

شما می‌توانید URL شروع را با هر وب‌سایت دلخواه جایگزین کنید.

## خروجی‌ها

پس از اتمام خزش، دو فایل ایجاد می‌شود:

1. `report.txt`: گزارش جامع شامل:
   - تاریخ و زمان خزش
   - دامنه اصلی
   - تعداد صفحات بازدید شده
   - تعداد لینک‌های داخلی و خارجی یافت شده
   - صفحات خطا (مثلاً 404)
   - ساختار درختی سایت به صورت خوانا

2. `tree.json`: ساختار درختی سایت به فرمت JSON، مناسب برای تحلیل‌های بعدی یا ویژوال‌سازی.

## تنظیمات پروژه

- حداکثر عمق خزش: 3 سطح (قابل تغییر در فایل `settings.py`)
- خزش فقط در دامنه مشخص شده (لینک‌های خارجی دنبال نمی‌شوند)
- تاخیر بین درخواست‌ها: 0.5 ثانیه (قابل تغییر در فایل `settings.py`) 